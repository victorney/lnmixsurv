// -*- mode: C++; c-indent-level: 4; c-basic-offset: 4; indent-tabs-mode: nil; -*-

#include <omp.h>
#include <RcppEigen.h>
#include <RcppGSL.h>
#include <gsl/gsl_rng.h>
#include <gsl/gsl_randist.h>

// [[Rcpp::plugins(openmp)]]
// [[Rcpp::depends(RcppEigen)]]
// [[Rcpp::depends(RcppGSL)]]

using namespace Rcpp;

// ------ RNG Framework ------
// Function to initialize the GSL random number generator
void initializeRNG(const long long int& seed, gsl_rng* rng_device) {
  gsl_rng_set(rng_device, seed);
}

void setSeed(const long long int& seed, gsl_rng* rng_device) {
  initializeRNG(seed, rng_device);
}

double runif_0_1(gsl_rng* rng_device) {
  return gsl_rng_uniform(rng_device);
}

double rnorm_(const double& mu, const double& sd, gsl_rng* rng_device) {
  return gsl_ran_gaussian(rng_device, sd) + mu;
}

double rgamma_(const double& alpha, const double& beta, gsl_rng* rng_device) {
  return gsl_ran_gamma(rng_device, alpha, 1.0 / beta);
}

// Sample one value (k-dimensional) from a 
// Dirichlet(alpha_1, alpha_2, ..., alpha_k)
Eigen::VectorXd rdirichlet(const Eigen::VectorXd& alpha, gsl_rng* rng_device) {
  int K = alpha.size();
  Eigen::VectorXd sample(K);
  
  for (int k = 0; k < K; ++k) {
    sample(k) = rgamma_(alpha(k), 1.0, rng_device);
  }
  
  sample /= sample.sum();
  return sample;
}

Eigen::VectorXd rmvnorm(const Eigen::VectorXd& mean,
                        const Eigen::MatrixXd& covariance, 
                        gsl_rng* rng_device) {
  int numDims = mean.size();
  Eigen::VectorXd sample(numDims);
  
  Eigen::LLT<Eigen::MatrixXd> lltOfCovariance(covariance);
  Eigen::MatrixXd L = lltOfCovariance.matrixL();
  
  Eigen::VectorXd Z(numDims);
  
  for (int j = 0; j < numDims; j++) {
    Z(j) = rnorm_(0, 1, rng_device);
  }
  
  sample = mean + L * Z;
  return sample;
}

/* AUXILIARY FUNCTIONS */
// Creates a sequence from start to end with 1 step
Eigen::VectorXi seq(const int& start, const int& end) {
  return Eigen::VectorXi::LinSpaced(end - start + 1, start, end);
}

// Function for replicating a numeric value K times.
Eigen::VectorXd repl(const double& x, const int& times) {
  return Eigen::VectorXd::Constant(times, x);
}

// Function to find which elements of the vector x are equal to g
Eigen::VectorXi findIndex(const Eigen::VectorXi& x,
                          const int& g) {
  Eigen::Array<bool, Eigen::Dynamic, 1> foundIndices = (x.array() == g).cast<bool>();
  
  int count = foundIndices.count();
  
  Eigen::VectorXi indices(count);
  int index = 0;
  
  for (int i = 0; i < foundIndices.size(); ++i) {
    if (foundIndices(i)) {
      indices(index) = i;
      ++index;
    }
  }
  
  return indices;
}

// Function used to create a diagonal matrix from a vector
Eigen::MatrixXd diagMat(Eigen::VectorXd diagonalVector) {
  Eigen::DiagonalMatrix<double, Eigen::Dynamic> diagonalMatrix = diagonalVector.asDiagonal();
  return diagonalMatrix;
}

Eigen::SparseMatrix<double> diagSparseMat(Eigen::VectorXd diagonalVector) {
  Eigen::SparseMatrix<double> sparseMat(diagonalVector.size(), diagonalVector.size());
  for (int i = 0; i < diagonalVector.size(); ++i) {
    sparseMat.insert(i, i) = diagonalVector(i);
  }
  
  return sparseMat;
}


// Function to create a submatrix with the rows within the variable index
Eigen::MatrixXd createSubmatrix(const Eigen::MatrixXd& X, 
                                const Eigen::VectorXi& index) {
  Eigen::MatrixXd submatrix(index.size(), X.cols());
  
  for (int i = 0; i < index.size(); ++i) {
    submatrix.row(i) = X.row(index(i));
  }
  
  return submatrix;
}

// Function to create a subvector with the rows within the variable index
Eigen::VectorXd createSubvector(const Eigen::VectorXd& x, const Eigen::VectorXi& index) {
  Eigen::VectorXd subvector(index.size());
  
  for (int i = 0; i < index.size(); ++i) {
    subvector(i) = x(index(i));
  }
  
  return subvector;
}

Eigen::VectorXi createSubvectorInt(const Eigen::VectorXi& x,
                                   const Eigen::VectorXi& index) {
  Eigen::VectorXi subvector(index.size());
  
  for (int i = 0; i < index.size(); ++i) {
    subvector(i) = x(index(i));
  }
  
  return subvector;
}

// Sample a random object from a given vector
// for now, just sample numeric objects (because of c++ class definition)
// and just one object.
int numeric_sample(const Eigen::VectorXi& groups,
                   const Eigen::VectorXd& probs,
                   gsl_rng* rng_device) {
  double u = runif_0_1(rng_device);
  double cumulativeProb = 0.0;
  int n = probs.size();
  for (int i = 0; i < n; ++i) {
    cumulativeProb += probs(i);
    
    if (u <= cumulativeProb) {
      return groups(i);
    }
  }
  
  // This point should never be reached and it's here just for compiling issues
  return 0;
}

// Function used to sample the latent groups for each observation. This one is
// the one that causes the code to run SO slow. We can further investigate what
// can be done in here
Eigen::VectorXi sample_groups(const int& G, const Eigen::VectorXd& y, const Eigen::VectorXd& eta, 
                         const Eigen::VectorXd& phi, const Eigen::MatrixXd& beta,
                         const Eigen::MatrixXd& X, gsl_rng* rng_device) {
  Eigen::VectorXi vec_grupos(y.size());
  
  Eigen::VectorXd probs(G);
  
  Eigen::MatrixXd mean = X * beta.transpose();
  
  Eigen::VectorXd sd(phi.size());
  
  for (int i = 0; i < phi.size(); i++) {
    sd(i) = 1.0 / sqrt(phi(i));
  }
  
  double denom;
  int n = y.size();
  
  for (int i = 0; i < n; i++) {
    denom = 0;
    
    for (int g = 0; g < G; g++) {
      probs(g) = eta(g) * R::dnorm(y(i), mean(i, g), sd(g), false);
      denom += probs(g); 
    }
    
    probs = (denom == 0) * (repl(1.0 / G, G)) +
      (denom != 0) * (probs / denom);
    
    vec_grupos(i) = numeric_sample(seq(0, G - 1), probs, rng_device);
  }
  
  return vec_grupos;
}

// Function used to simulate survival time for censored observations.
// Here, delta is a vector such that delta(i) == 0 implies that the 
// i-th observation is censored. Otherwise, the i-th observation is a
// failure time.

// This function can be vectorized with a little of work, which can
// improve performance. Take a look at
// https://cran.r-project.org/web/packages/RcppTN/RcppTN.pdf
// for sampling multiples numbers at the same time.
Eigen::VectorXd augment(int G, const Eigen::VectorXd& y, const Eigen::VectorXi& groups,
                  const Eigen::VectorXi& delta, const Eigen::VectorXd& phi, 
                  const Eigen::MatrixXd& beta, const Eigen::MatrixXd& X, gsl_rng* rng_device) {
  
  int n = X.rows();
  
  Eigen::VectorXd out(n);
  int g;
  double out_i;
  
  for (int i = 0; i < n; i++) {
    if (delta(i) == 1) {
      out(i) = y(i);
    }
    
    else {
      g = groups(i);
      
      out_i = y(i);
      int count = 0;
      while(out_i <= y(i)) {
        out_i = rnorm_(X.row(i) * beta.row(g).transpose(),
                       sqrt(1.0 / phi(g)), rng_device);
        
        // break if it's going to run forever
        // should never happen
        if(count > 100000) {
          out_i = y(i) + 0.1;
          break;
        }
        count ++;
      }
      
      out(i) = out_i;
    }
  }
  
  return out;
}

// Create a table for each numeric element in the vector groups.
// For now, we are just going to use to evaluate how much observations
// we have at each group, given a vector of latent groups.
Eigen::VectorXi groups_table(const int& G, 
                             const Eigen::VectorXi& groups) {
  Eigen::VectorXi out(G);
  Eigen::VectorXi index;
  
  for (int g = 0; g < G; g++) {
    index = createSubvectorInt(groups, findIndex(groups, g));
    
    out(g) = index.rows();
  }
  
  return(out);
}

Eigen::MatrixXd makeSymmetric(const Eigen::MatrixXd X) {
  Eigen::MatrixXd out(X.rows(), X.cols());
  int rows = X.rows();
  int cols = X.cols();
  for(int r = 0; r < rows; r++) {
    for(int c = 0; c < cols; c++) {
      out(r, c) = X(r, c);
      out(c, r) = X(r, c);
    }
  }
  
  return out;
}

Eigen::MatrixXd lognormal_mixture_gibbs_implementation(int Niter, int em_iter, int G, 
                                                 Eigen::VectorXd exp_y, Eigen::VectorXi delta, 
                                                 Eigen::MatrixXd X, double a, 
                                                 long long int starting_seed,
                                                 bool show_output) {
  
  gsl_rng* global_rng = gsl_rng_alloc(gsl_rng_default);
  
  // setting global seed to start the sampler
  setSeed(starting_seed, global_rng);
  
  // add verifications for robustiness. Skipping for the sake of simplicity.
  
  // Calculating number of columns of the output matrix:
  // Each group has p (#cols X) covariates, 1 mixture component and
  // 1 precision. This implies:
  int p = X.cols();
  int nColsOutput = (p + 2) * G;
  int N = X.rows();
  
  Eigen::VectorXd y(exp_y.size());
  
  for(int i = 0; i < exp_y.size(); i++) {
    y(i) = log(exp_y(i));
  }
  
  // The output matrix should have Niter rows (1 row for each iteration) and
  // nColsOutput columns (1 column for each element).
  Eigen::MatrixXd out(Niter, nColsOutput);
  
  // The order of filling the output matrix matters a lot, since we can
  // make label switching accidentally. Latter this is going to be defined
  // so we can always fill the matrix in the correct order (by columns, always).
  
  // Starting empty objects for EM
  Eigen::VectorXd eta_em(G);
  Eigen::MatrixXd Xt = X.transpose();
  Eigen::VectorXd phi_em(G);
  Eigen::VectorXd y_aug(N);
  Eigen::MatrixXd beta_em(G, p);
  Eigen::MatrixXd tau(N, G);
  Eigen::SparseMatrix<double> Wg(N, N);
  Eigen::VectorXi n_groups(G);
  Eigen::MatrixXd means(N, G);
  Eigen::VectorXd sd(G);
  Eigen::VectorXd linearComb(N);
  Eigen::RowVectorXd newRow(nColsOutput);
  
  double sumtau;
  
  // starting EM algorithm to find values close to the MLE
  for (int iter = 0; iter < em_iter; iter++) {
    
    if ((iter % 50 == 0) && show_output) {
      Rcout << "EM Iter: " << iter << "/" << em_iter << "\n";
    }
    
    // Initializing values
    if(iter == 0) {
      eta_em = rdirichlet(repl(1, G), global_rng);
      
      for (int g = 0; g < G; g++) {
        phi_em(g) = rgamma_(2, 8, global_rng);
        beta_em.row(g) = rmvnorm(repl(0, p),
                    diagMat(repl(7, p)), global_rng).transpose();
      }
    }
    
    // E-step
    means = X * beta_em.transpose();
    
    for(int c = 0; c < phi_em.size(); c++) {
      sd(c) = 1.0 / sqrt(phi_em(c));
    }
    
    for(int r = 0; r < N; r++) {
      for(int g = 0; g < G; g++) {
        tau(r, g) = eta_em(g) * R::dnorm(y(r), 
            means(r, g), sd(g), false);
      }
      
      if(tau.row(r).sum() == 0) {  // to avoid numerical problems
        tau.row(r) = repl(1.0/G, G).transpose();
      } else {
        tau.row(r) = tau.row(r)/tau.row(r).sum();
      }
    }
    
    // M-step
    for(int g = 0; g < G; g++) {
      Wg = diagSparseMat(tau.col(g));
      sumtau = tau.col(g).sum();
      
      eta_em(g) = sumtau/N;
      
      if((X.transpose() * Wg * X).determinant() != 0) {
        beta_em.row(g) = ((Xt * Wg * X).inverse() * Xt * Wg * y).transpose();
      }
      
      linearComb = y - X * beta_em.row(g).transpose();
      
      phi_em(g) = sumtau/(linearComb.transpose() * Wg * linearComb);
    }
  }
  
  // Starting other new values for MCMC algorithms
  Eigen::VectorXd eta(G);
  Eigen::VectorXd phi(G);
  Eigen::MatrixXd beta(G, p);
  Eigen::VectorXi groups(y.rows());
  Eigen::VectorXd e0new_vec(2);
  
  Eigen::MatrixXd Xg;
  Eigen::MatrixXd Xgt;
  Eigen::VectorXd yg;
  
  Eigen::VectorXi indexg;
  Eigen::MatrixXd Sg(p, p);
  Eigen::VectorXd mg(p);
  Eigen::VectorXd log_eta_new(G);
  
  double e0;
  double e0_prop;
  double log_alpha;
  double b;
  double cte;
  double prop_aceite;
  int n_aceite;
  double count = 0;
  
  for (int iter = 0; iter < Niter; iter++) {
    // Starting empty objects for Gibbs Sampler
    if (iter == 0) { 
      // we are going to start the values using the
      // previous EM iteration
      eta = eta_em;
      phi = phi_em;
      beta = beta_em;
      
      // sampling value for e0
      e0 = rgamma_(1, 1, global_rng);
      
      // defining values for sintonizing the variance
      // of e0 proposal
      cte = 1;
      n_aceite = 0;
      
      // Sampling classes for the observations
      groups = sample_groups(G, y, eta, phi, beta, X, global_rng);
    }
    
    // Data augmentation
    y_aug = augment(G, y, groups, delta, phi, beta, X, global_rng);
    
    // Sampling classes for the observations
    groups = sample_groups(G, y_aug, eta, phi, beta, X, global_rng);
    
    // Computing number of observations allocated at each class
    n_groups = groups_table(G, groups);
    
    // ensuring that every class have, at least, 2 observations
    for(int g = 0; g < G; g++) {
      if(n_groups(g) == 0) {
        for(int m = 0; m < 2; m++) {
          Eigen::VectorXi sequence = seq(0, N);
          int idx = numeric_sample(sequence,
                                   repl(1.0 / X.rows(), X.rows()),
                                   global_rng);
          groups(idx) = g;
        }
        
        // recalculating the number of groups
        n_groups = groups_table(G, groups);
      }
    }
    
    // Sampling new eta
    eta = rdirichlet(n_groups.cast<double>() + repl(e0, G), global_rng);
    
    // For each g, sample new phi[g] and beta[g, _]
    for (int g = 0; g < G; g++) {
      indexg = findIndex(groups, g);
      
      if(true) {
        Xg = createSubmatrix(X, indexg);
        yg = createSubvector(y_aug, indexg);
        Xgt = Xg.transpose();
        
        linearComb = yg - Xg * beta.row(g).transpose();
        double quadLinearComb = linearComb.transpose().dot(linearComb);
        
        // sampling phi new
        // the priori used was Gamma(0.001, 0.001)
        
        phi(g) = rgamma_(n_groups(g) / 2.0 + 0.001, (1.0 / 2) *
          quadLinearComb + 0.001, global_rng);
        
        // sampling beta new
        // the priori used was MNV(vec 0, diag 1000)
        if((phi(g) * Xgt * Xg + 
           diagMat(repl(1.0 / 1000, p))).determinant() != 0) {
          Sg = (phi(g) * Xgt * Xg + diagMat(repl(1.0 / 1000, p))).inverse();
          
          Sg = makeSymmetric(Sg);
          
          mg = Sg * (phi(g) * Xgt * yg);
          
          beta.row(g) = rmvnorm(mg, Sg, global_rng).transpose();
        }
      }
    }
    
    // atualizando o valor da constante de sintonização
    // cte a cada 200 iterações
    if ((iter % 200) == 0) {
      prop_aceite = n_aceite/200.0;
      
      if (prop_aceite > 0.25) {
        cte = cte/((2*sqrt(3)-2)/(1+exp(0.04*count)) + 1);
      } else if (prop_aceite < 0.17) {
        cte = cte*((2*sqrt(3)-2)/(1+exp(0.04*count)) + 1);
      }
      // ((2*sqrt(3)-2)/(1+exp(0.04*count)) + 1) é um valor
      // de correção que converge para 1 quando count -> Inf e,
      // quando count = 0, o resultado é sqrt(3).
      
      // o valor 0.04 representa a velocidade da convergência
      // para 1 e sqrt(3), o valor em count = 0, foi definido
      // arbitrariamente.
      
      n_aceite = 0;
      count = count + 1.0;
    }
    
    b = cte*a;
    
    // updating the value of e0 (eta's dirichlet hyperparameter)
    e0_prop = rgamma_(b*e0, b, global_rng);
    
    for(int c = 0; c < eta.size(); c++) {
      log_eta_new(c) = log(eta(c));
    }
    
    
    log_alpha = (log_eta_new).sum()*(e0_prop - e0) +
      9.0 * log(e0_prop/e0) - 10.0*G*(e0_prop - e0) +
      b*(e0_prop - e0) + (b*e0_prop - 1.0)*log(e0) -
      (b*e0 - 1.0)*log(e0_prop);
    
    double u = runif_0_1(global_rng);
    e0 = (log(u) < log_alpha) * e0_prop +
      (log(u) >= log_alpha) * e0;
    
    n_aceite += (log(u) < log_alpha);
    
    // filling the ith iteration row of the output matrix
    // the order of filling will always be the following:
    
    // First Mixture: proportion, betas, phi
    // Second Mixture: proportion, betas, phi
    // ...
    // Last Mixture: proportion, betas, phi
    
    // arma::uvec sorteta = arma::sort_index(eta, "descend");
    // beta = beta.rows(sorteta);
    // phi = phi.rows(sorteta);
    // eta = eta.rows(sorteta);
    
    for(int g = 0; g < G; g++) {
      newRow(g*(p + 2)) = eta(g);
      
      for(int c = 0; c < p; c++) {
        newRow(g*(p + 2) + c + 1) = beta(g, c);
      }
      
      newRow(g*(p + 2) + (p + 1)) = phi(g);
      
      if(g == (G - 1)) {
        newRow(nColsOutput - 1) = e0;
      }
    }
    
    out.row(iter) = newRow;
    
    if((iter % 500 == 0) && show_output) {
      Rcout << "MCMC Iter: " << iter << "/" << Niter << "\n";
    }
  }
  
  if(show_output) {
    Rcout << "Done" << "\n";
  }
  return out;
}

// Function to call lognormal_mixture_gibbs_implementation with parallellization
// [[Rcpp::export]]
Eigen::MatrixXd lognormal_mixture_gibbs(int Niter, int em_iter, int G, Eigen::VectorXd exp_y, Eigen::VectorXi delta, Eigen::MatrixXd X,
                                      double a, Eigen::Array<long long int, Eigen::Dynamic, 1> starting_seed,
                                      bool show_output, int n_cores, int n_chains) {
  
  const int numCols = (X.cols() + 2) * G;      // Number of columns in each matrix
  
  Eigen::MatrixXd out(Niter * n_chains, numCols);
  
  if(n_cores == 1) {
    for(int chain = 0; chain < n_chains; chain ++) {
      Eigen::MatrixXd gibbs_sampled = lognormal_mixture_gibbs_implementation(Niter, em_iter, G, exp_y, delta, X, a, starting_seed(chain), show_output);
      for (int r = 0; r < gibbs_sampled.rows(); r++) {
        out.row(r + Niter * chain) = gibbs_sampled.row(r);
      }
    }
    
    return out;
  }
  
  omp_set_dynamic(0); // related to https://stackoverflow.com/questions/11095309/openmp-set-num-threads-is-not-working
  omp_set_num_threads(n_cores);
  
  int chain;
  #pragma omp parallel for private(chain)
  for(int chain = 0; chain < n_chains; chain ++) {
    int desloc = chain * Niter;
    Eigen::MatrixXd gibbs_sampled = lognormal_mixture_gibbs_implementation(Niter, em_iter, G, exp_y, delta, X, a, starting_seed(chain), show_output);
    for (int r = 0; r < gibbs_sampled.rows(); r++) {
      out.row(r + desloc) = gibbs_sampled.row(r);
    }
  }
  
  return out;
}